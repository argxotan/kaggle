{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/eikedehling/tune-and-compare-xgb-lightgbm-rf-with-hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy\n",
    "import catboost\n",
    "import hyperopt\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "import colorama\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train and test files\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['target']\n",
    "X = train_df.drop(['ID_code', 'target'], axis=1)\n",
    "\n",
    "X_test = test_df.drop(['ID_code'], axis=1)\n",
    "X_test.fillna(999, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.8,  random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(truth, predictions):\n",
    "    g = np.asarray(np.c_[truth, predictions, np.arange(len(truth)) ], dtype=np.float)\n",
    "    g = g[np.lexsort((g[:,2], -1*g[:,1]))]\n",
    "    gs = g[:,0].cumsum().sum() / g[:,0].sum()\n",
    "    gs -= (len(truth) + 1) / 2.\n",
    "    return gs / len(truth)\n",
    "\n",
    "def gini_xgb(predictions, truth):\n",
    "    truth = truth.get_label()\n",
    "    return 'gini', -1.0 * gini(truth, predictions) / gini(truth, truth)\n",
    "\n",
    "def gini_lgb(truth, predictions):\n",
    "    score = gini(truth, predictions) / gini(truth, truth)\n",
    "    return 'gini', score, True\n",
    "\n",
    "def gini_sklearn(truth, predictions):\n",
    "    return gini(truth, predictions) / gini(truth, truth)\n",
    "\n",
    "gini_scorer = make_scorer(gini_sklearn, greater_is_better=True, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini 0.782 params {'n_estimators': 300, 'max_depth': 5}\n",
      "Gini 0.768 params {'n_estimators': 450, 'max_depth': 2}\n",
      "Gini 0.789 params {'n_estimators': 375, 'max_depth': 6}\n",
      "Gini 0.772 params {'n_estimators': 100, 'max_depth': 4}\n",
      "Gini 0.803 params {'n_estimators': 300, 'max_depth': 9}\n",
      "Gini 0.755 params {'n_estimators': 325, 'max_depth': 1}\n",
      "Gini 0.766 params {'n_estimators': 300, 'max_depth': 2}\n",
      "Gini 0.778 params {'n_estimators': 50, 'max_depth': 6}\n",
      "Gini 0.795 params {'n_estimators': 400, 'max_depth': 7}\n",
      "Gini 0.766 params {'n_estimators': 50, 'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "def objective(params):\n",
    "    params = {'n_estimators': int(params['n_estimators']), \n",
    "              'max_depth': int(params['max_depth'])}\n",
    "    \n",
    "    clf = RandomForestClassifier(n_jobs=4, class_weight={0:1, 1:8}, **params)\n",
    "    \n",
    "    score = cross_val_score(clf, X, y, scoring='roc_auc', cv=StratifiedKFold(n_splits = 5)).mean()\n",
    "    print(\"Gini {:.3f} params {}\".format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 25, 500, 25),\n",
    "    'max_depth': hp.quniform('max_depth', 1, 10, 1)\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperopt estimated optimum {'max_depth': 1.0, 'n_estimators': 325.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Hyperopt estimated optimum {}\".format(best))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_jobs=4, class_weight={0:1, 1:8}, n_estimators = 325, max_depth = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 8},\n",
       "            criterion='gini', max_depth=1, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=325, n_jobs=4, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_cv = cross_val_score(rfc, X, y, scoring='roc_auc', cv=StratifiedKFold(n_splits = 5)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone \n",
    "\n",
    "def drop_col_feat_imp(model, X_train, y_train, random_state = 42):\n",
    "    \n",
    "    # clone the model to have the exact same specification as the one initially trained\n",
    "    model_clone = clone(model)\n",
    "    # set random_state for comparability\n",
    "    model_clone.random_state = random_state\n",
    "    # training and scoring the benchmark model\n",
    "    model_clone.fit(X_train, y_train)\n",
    "    benchmark_score = model_clone.score(X_train, y_train)\n",
    "    # list for storing feature importances\n",
    "    importances = []\n",
    "    \n",
    "    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n",
    "    for col in X_train.columns:\n",
    "        model_clone = clone(model)\n",
    "        model_clone.random_state = random_state\n",
    "        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n",
    "        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n",
    "        importances.append(benchmark_score - drop_col_score)\n",
    "    \n",
    "    importances_df = imp_df(X_train.columns, importances)\n",
    "    return importances_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-c11661944fcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdrop_col_feat_imp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-72-30fb107ddd53>\u001b[0m in \u001b[0;36mdrop_col_feat_imp\u001b[0;34m(model, X_train, y_train, random_state)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimportances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbenchmark_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdrop_col_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimportances_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimportances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimportances_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imp_df' is not defined"
     ]
    }
   ],
   "source": [
    "drop_col_feat_imp(rfc, X_train, y_train, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rfc.feature_importances_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00615385])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances[1:2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 53 (0.073846)\n",
      "2. feature 81 (0.070769)\n",
      "3. feature 12 (0.058462)\n",
      "4. feature 139 (0.058462)\n",
      "5. feature 110 (0.058462)\n",
      "6. feature 146 (0.055385)\n",
      "7. feature 26 (0.055385)\n",
      "8. feature 22 (0.049231)\n",
      "9. feature 174 (0.036923)\n",
      "10. feature 109 (0.033846)\n",
      "11. feature 80 (0.033846)\n",
      "12. feature 6 (0.024615)\n",
      "13. feature 99 (0.021538)\n",
      "14. feature 76 (0.021538)\n",
      "15. feature 44 (0.021538)\n",
      "16. feature 21 (0.021538)\n",
      "17. feature 166 (0.021538)\n",
      "18. feature 165 (0.018462)\n",
      "19. feature 198 (0.015385)\n",
      "20. feature 40 (0.015385)\n",
      "21. feature 133 (0.015385)\n",
      "22. feature 2 (0.015385)\n",
      "23. feature 0 (0.015385)\n",
      "24. feature 179 (0.015385)\n",
      "25. feature 13 (0.012308)\n",
      "26. feature 115 (0.012308)\n",
      "27. feature 164 (0.012308)\n",
      "28. feature 33 (0.009231)\n",
      "29. feature 78 (0.009231)\n",
      "30. feature 123 (0.009231)\n",
      "31. feature 191 (0.009231)\n",
      "32. feature 177 (0.009231)\n",
      "33. feature 92 (0.009231)\n",
      "34. feature 190 (0.006154)\n",
      "35. feature 170 (0.006154)\n",
      "36. feature 184 (0.006154)\n",
      "37. feature 1 (0.006154)\n",
      "38. feature 108 (0.006154)\n",
      "39. feature 9 (0.006154)\n",
      "40. feature 89 (0.006154)\n",
      "41. feature 91 (0.006154)\n",
      "42. feature 34 (0.003077)\n",
      "43. feature 147 (0.003077)\n",
      "44. feature 119 (0.003077)\n",
      "45. feature 95 (0.003077)\n",
      "46. feature 148 (0.003077)\n",
      "47. feature 154 (0.003077)\n",
      "48. feature 86 (0.003077)\n",
      "49. feature 188 (0.003077)\n",
      "50. feature 173 (0.003077)\n",
      "51. feature 56 (0.003077)\n",
      "52. feature 51 (0.000000)\n",
      "53. feature 57 (0.000000)\n",
      "54. feature 52 (0.000000)\n",
      "55. feature 189 (0.000000)\n",
      "56. feature 50 (0.000000)\n",
      "57. feature 54 (0.000000)\n",
      "58. feature 49 (0.000000)\n",
      "59. feature 48 (0.000000)\n",
      "60. feature 55 (0.000000)\n",
      "61. feature 176 (0.000000)\n",
      "62. feature 58 (0.000000)\n",
      "63. feature 68 (0.000000)\n",
      "64. feature 75 (0.000000)\n",
      "65. feature 74 (0.000000)\n",
      "66. feature 73 (0.000000)\n",
      "67. feature 72 (0.000000)\n",
      "68. feature 71 (0.000000)\n",
      "69. feature 70 (0.000000)\n",
      "70. feature 69 (0.000000)\n",
      "71. feature 67 (0.000000)\n",
      "72. feature 59 (0.000000)\n",
      "73. feature 66 (0.000000)\n",
      "74. feature 65 (0.000000)\n",
      "75. feature 64 (0.000000)\n",
      "76. feature 63 (0.000000)\n",
      "77. feature 62 (0.000000)\n",
      "78. feature 61 (0.000000)\n",
      "79. feature 60 (0.000000)\n",
      "80. feature 47 (0.000000)\n",
      "81. feature 39 (0.000000)\n",
      "82. feature 46 (0.000000)\n",
      "83. feature 11 (0.000000)\n",
      "84. feature 19 (0.000000)\n",
      "85. feature 18 (0.000000)\n",
      "86. feature 17 (0.000000)\n",
      "87. feature 16 (0.000000)\n",
      "88. feature 15 (0.000000)\n",
      "89. feature 14 (0.000000)\n",
      "90. feature 195 (0.000000)\n",
      "91. feature 10 (0.000000)\n",
      "92. feature 194 (0.000000)\n",
      "93. feature 8 (0.000000)\n",
      "94. feature 7 (0.000000)\n",
      "95. feature 196 (0.000000)\n",
      "96. feature 5 (0.000000)\n",
      "97. feature 4 (0.000000)\n",
      "98. feature 3 (0.000000)\n",
      "99. feature 197 (0.000000)\n",
      "100. feature 20 (0.000000)\n",
      "101. feature 193 (0.000000)\n",
      "102. feature 45 (0.000000)\n",
      "103. feature 32 (0.000000)\n",
      "104. feature 43 (0.000000)\n",
      "105. feature 42 (0.000000)\n",
      "106. feature 41 (0.000000)\n",
      "107. feature 38 (0.000000)\n",
      "108. feature 37 (0.000000)\n",
      "109. feature 36 (0.000000)\n",
      "110. feature 35 (0.000000)\n",
      "111. feature 31 (0.000000)\n",
      "112. feature 23 (0.000000)\n",
      "113. feature 30 (0.000000)\n",
      "114. feature 29 (0.000000)\n",
      "115. feature 28 (0.000000)\n",
      "116. feature 27 (0.000000)\n",
      "117. feature 192 (0.000000)\n",
      "118. feature 25 (0.000000)\n",
      "119. feature 24 (0.000000)\n",
      "120. feature 77 (0.000000)\n",
      "121. feature 88 (0.000000)\n",
      "122. feature 79 (0.000000)\n",
      "123. feature 181 (0.000000)\n",
      "124. feature 180 (0.000000)\n",
      "125. feature 145 (0.000000)\n",
      "126. feature 144 (0.000000)\n",
      "127. feature 143 (0.000000)\n",
      "128. feature 142 (0.000000)\n",
      "129. feature 141 (0.000000)\n",
      "130. feature 140 (0.000000)\n",
      "131. feature 138 (0.000000)\n",
      "132. feature 150 (0.000000)\n",
      "133. feature 137 (0.000000)\n",
      "134. feature 136 (0.000000)\n",
      "135. feature 135 (0.000000)\n",
      "136. feature 134 (0.000000)\n",
      "137. feature 182 (0.000000)\n",
      "138. feature 132 (0.000000)\n",
      "139. feature 131 (0.000000)\n",
      "140. feature 149 (0.000000)\n",
      "141. feature 151 (0.000000)\n",
      "142. feature 129 (0.000000)\n",
      "143. feature 162 (0.000000)\n",
      "144. feature 172 (0.000000)\n",
      "145. feature 171 (0.000000)\n",
      "146. feature 169 (0.000000)\n",
      "147. feature 168 (0.000000)\n",
      "148. feature 167 (0.000000)\n",
      "149. feature 178 (0.000000)\n",
      "150. feature 163 (0.000000)\n",
      "151. feature 161 (0.000000)\n",
      "152. feature 152 (0.000000)\n",
      "153. feature 160 (0.000000)\n",
      "154. feature 159 (0.000000)\n",
      "155. feature 158 (0.000000)\n",
      "156. feature 157 (0.000000)\n",
      "157. feature 156 (0.000000)\n",
      "158. feature 155 (0.000000)\n",
      "159. feature 153 (0.000000)\n",
      "160. feature 130 (0.000000)\n",
      "161. feature 128 (0.000000)\n",
      "162. feature 187 (0.000000)\n",
      "163. feature 93 (0.000000)\n",
      "164. feature 101 (0.000000)\n",
      "165. feature 100 (0.000000)\n",
      "166. feature 185 (0.000000)\n",
      "167. feature 98 (0.000000)\n",
      "168. feature 97 (0.000000)\n",
      "169. feature 96 (0.000000)\n",
      "170. feature 94 (0.000000)\n",
      "171. feature 90 (0.000000)\n",
      "172. feature 103 (0.000000)\n",
      "173. feature 175 (0.000000)\n",
      "174. feature 87 (0.000000)\n",
      "175. feature 85 (0.000000)\n",
      "176. feature 84 (0.000000)\n",
      "177. feature 83 (0.000000)\n",
      "178. feature 82 (0.000000)\n",
      "179. feature 186 (0.000000)\n",
      "180. feature 102 (0.000000)\n",
      "181. feature 104 (0.000000)\n",
      "182. feature 127 (0.000000)\n",
      "183. feature 117 (0.000000)\n",
      "184. feature 126 (0.000000)\n",
      "185. feature 125 (0.000000)\n",
      "186. feature 124 (0.000000)\n",
      "187. feature 122 (0.000000)\n",
      "188. feature 121 (0.000000)\n",
      "189. feature 120 (0.000000)\n",
      "190. feature 118 (0.000000)\n",
      "191. feature 116 (0.000000)\n",
      "192. feature 105 (0.000000)\n",
      "193. feature 114 (0.000000)\n",
      "194. feature 113 (0.000000)\n",
      "195. feature 112 (0.000000)\n",
      "196. feature 111 (0.000000)\n",
      "197. feature 183 (0.000000)\n",
      "198. feature 107 (0.000000)\n",
      "199. feature 106 (0.000000)\n",
      "200. feature 199 (0.000000)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importances = rfc.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in rfc.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    params = {\n",
    "        'max_depth': int(params['max_depth']),\n",
    "       'n_estimators': int(params['n_estimators']),\n",
    "     #   'gamma': \"{:.3f}\".format(params['gamma']),\n",
    "      #  'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n",
    "    }\n",
    "    \n",
    "    clf = xgb.XGBClassifier(\n",
    "        #n_estimators=250,\n",
    "        learning_rate=0.1,\n",
    "        class_weight={0:1, 1:8},\n",
    "        n_jobs=2,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(clf, X, y, scoring='roc_auc', cv=StratifiedKFold(n_splits = 5)).mean()\n",
    "    print(\"Gini {:.3f} params {}\".format(score, params))\n",
    "    return score\n",
    "\n",
    "space = {\n",
    "    'max_depth': hp.quniform('max_depth', 2, 8, 1),\n",
    "    'n_estimators': hp.quniform('n_estimators', 500, 2500, 100),\n",
    "   # 'colsample_bytree': hp.uniform('colsample_bytree', 0.3, 1.0),\n",
    "   # 'gamma': hp.uniform('gamma', 0.0, 0.5),\n",
    "}\n",
    "\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/dreeux/hyperparameter-tuning-using-hyperopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/eikedehling/tune-and-compare-xgb-lightgbm-rf-with-hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
